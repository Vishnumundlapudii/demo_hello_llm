{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Networks LLM Integration Tutorial\n",
    "\n",
    "## üìö Complete Guide to Using E2E Networks LLM with Python\n",
    "\n",
    "This notebook demonstrates step-by-step how to:\n",
    "1. Initialize and use E2E Networks LLM with LangChain\n",
    "2. Make direct API calls to E2E endpoints\n",
    "3. Handle responses and errors\n",
    "4. Compare different approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Environment Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import Optional, List, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "# For OpenAI-compatible approach\n",
    "import openai\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 2: Load Environment Variables\n",
    "\n",
    "Load our E2E Networks credentials from the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get E2E Networks credentials\n",
    "E2E_ENDPOINT_URL = os.getenv(\"E2E_ENDPOINT_URL\")\n",
    "E2E_API_KEY = os.getenv(\"E2E_API_KEY\")\n",
    "\n",
    "print(f\"Endpoint URL: {E2E_ENDPOINT_URL}\")\n",
    "print(f\"API Key: {'*' * 20}...{E2E_API_KEY[-10:] if E2E_API_KEY else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Method 1: Custom LangChain LLM Wrapper\n",
    "\n",
    "Let's create a custom LangChain LLM wrapper for E2E Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2ENetworksLLM(LLM):\n",
    "    \"\"\"Custom LLM wrapper for E2E Networks endpoint\"\"\"\n",
    "    \n",
    "    endpoint_url: str = \"\"\n",
    "    api_key: str = \"\"\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 1000\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.endpoint_url = E2E_ENDPOINT_URL\n",
    "        self.api_key = E2E_API_KEY\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"e2e_networks_llm\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call the E2E LLM endpoint using OpenAI-compatible format\"\"\"\n",
    "        \n",
    "        print(f\"üöÄ Making API call to: {self.endpoint_url}\")\n",
    "        print(f\"üìù Prompt: {prompt[:100]}...\")\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        # Use OpenAI-compatible chat completions format\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens\n",
    "        }\n",
    "        \n",
    "        if stop:\n",
    "            payload[\"stop\"] = stop\n",
    "        \n",
    "        try:\n",
    "            # Ensure endpoint ends with chat/completions\n",
    "            endpoint = self.endpoint_url\n",
    "            if not endpoint.endswith(\"chat/completions\"):\n",
    "                if endpoint.endswith(\"/\"):\n",
    "                    endpoint += \"chat/completions\"\n",
    "                else:\n",
    "                    endpoint += \"/chat/completions\"\n",
    "            \n",
    "            print(f\"üåê Full endpoint: {endpoint}\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                endpoint,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            print(f\"‚úÖ Response received: {len(str(result))} characters\")\n",
    "            \n",
    "            # Handle OpenAI-compatible response format\n",
    "            if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                choice = result[\"choices\"][0]\n",
    "                if \"message\" in choice:\n",
    "                    return choice[\"message\"].get(\"content\", \"\")\n",
    "                elif \"text\" in choice:\n",
    "                    return choice[\"text\"]\n",
    "            \n",
    "            return \"No valid response received\"\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            error_msg = f\"‚ùå Error calling E2E LLM: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Unexpected error: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "print(\"‚úÖ E2ENetworksLLM class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 3: Initialize and Test LangChain LLM\n",
    "\n",
    "Now let's create an instance of our custom LLM and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = E2ENetworksLLM(\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"üéØ LLM initialized with:\")\n",
    "print(f\"   Model: {llm.model_name}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")\n",
    "print(f\"   Max Tokens: {llm.max_tokens}\")\n",
    "print(f\"   LLM Type: {llm._llm_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM with a simple prompt\n",
    "test_prompt = \"What is Python programming language? Give a brief explanation.\"\n",
    "\n",
    "print(\"üß™ Testing LLM with prompt:\")\n",
    "print(f\"'{test_prompt}'\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "response = llm._call(test_prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ü§ñ LLM Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Method 2: Direct OpenAI-Compatible API Call\n",
    "\n",
    "Let's also demonstrate how to make direct API calls using the OpenAI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI client for E2E Networks\n",
    "openai.api_key = E2E_API_KEY\n",
    "openai.base_url = E2E_ENDPOINT_URL\n",
    "\n",
    "print(\"üîß OpenAI client configured for E2E Networks\")\n",
    "print(f\"Base URL: {openai.base_url}\")\n",
    "print(f\"API Key: {'*' * 20}...{E2E_API_KEY[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a direct API call using OpenAI format\n",
    "def call_e2e_llm_direct(prompt, temperature=0.7, max_tokens=500):\n",
    "    \"\"\"Direct API call to E2E Networks LLM\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Making direct API call...\")\n",
    "    print(f\"üìù Prompt: {prompt[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        response_content = completion.choices[0].message.content\n",
    "        print(f\"‚úÖ Response received: {len(response_content)} characters\")\n",
    "        \n",
    "        return response_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error in direct API call: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Test the direct API call\n",
    "direct_prompt = \"Write a Python function to calculate factorial of a number.\"\n",
    "\n",
    "print(\"üß™ Testing direct API call with prompt:\")\n",
    "print(f\"'{direct_prompt}'\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "direct_response = call_e2e_llm_direct(direct_prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ü§ñ Direct API Response:\")\n",
    "print(direct_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Compare Different Approaches\n",
    "\n",
    "Let's compare the LangChain wrapper vs direct API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test prompt for comparison\n",
    "comparison_prompt = \"Explain the difference between machine learning and deep learning in simple terms.\"\n",
    "\n",
    "print(\"üîÑ Comparing LangChain vs Direct API approaches...\")\n",
    "print(f\"Test prompt: '{comparison_prompt}'\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Method 1: LangChain\n",
    "print(\"\\nüîó Method 1: LangChain Wrapper\")\n",
    "start_time = time.time()\n",
    "langchain_response = llm._call(comparison_prompt)\n",
    "langchain_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Time taken: {langchain_time:.2f} seconds\")\n",
    "print(f\"üìù Response length: {len(langchain_response)} characters\")\n",
    "print(f\"ü§ñ Response: {langchain_response[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Method 2: Direct API\n",
    "print(\"\\nüåê Method 2: Direct OpenAI API\")\n",
    "start_time = time.time()\n",
    "direct_response = call_e2e_llm_direct(comparison_prompt)\n",
    "direct_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Time taken: {direct_time:.2f} seconds\")\n",
    "print(f\"üìù Response length: {len(direct_response)} characters\")\n",
    "print(f\"ü§ñ Response: {direct_response[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà Performance Comparison:\")\n",
    "print(f\"   LangChain: {langchain_time:.2f}s\")\n",
    "print(f\"   Direct API: {direct_time:.2f}s\")\n",
    "print(f\"   Difference: {abs(langchain_time - direct_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 5: Advanced Examples\n",
    "\n",
    "Let's explore more advanced usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Code generation\n",
    "code_prompt = \"\"\"\n",
    "Create a Python class for a simple bank account with the following methods:\n",
    "1. __init__(self, account_number, initial_balance)\n",
    "2. deposit(self, amount)\n",
    "3. withdraw(self, amount)\n",
    "4. get_balance(self)\n",
    "\n",
    "Include proper error handling and documentation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíª Code Generation Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "code_response = llm._call(code_prompt)\n",
    "print(code_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Creative writing\n",
    "creative_prompt = \"\"\"\n",
    "Write a short story (3-4 paragraphs) about a programmer who discovers \n",
    "that their AI assistant has developed consciousness. Make it engaging and thought-provoking.\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úçÔ∏è Creative Writing Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "creative_response = call_e2e_llm_direct(creative_prompt, temperature=0.9, max_tokens=800)\n",
    "print(creative_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Technical explanation\n",
    "technical_prompt = \"\"\"\n",
    "Explain how neural networks work, including:\n",
    "1. Basic structure (neurons, layers)\n",
    "2. Forward propagation\n",
    "3. Backpropagation\n",
    "4. Training process\n",
    "\n",
    "Use simple language suitable for beginners.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß† Technical Explanation Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "technical_response = llm._call(technical_prompt)\n",
    "print(technical_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 6: Error Handling and Best Practices\n",
    "\n",
    "Let's demonstrate proper error handling and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_llm_call(prompt, max_retries=3, timeout=30):\n",
    "    \"\"\"Robust LLM call with retry logic and error handling\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"üîÑ Attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Use the LangChain wrapper\n",
    "            response = llm._call(prompt)\n",
    "            \n",
    "            # Validate response\n",
    "            if response and not response.startswith(\"Error\"):\n",
    "                print(f\"‚úÖ Success on attempt {attempt + 1}\")\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Invalid response on attempt {attempt + 1}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error on attempt {attempt + 1}: {str(e)}\")\n",
    "            \n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"‚è≥ Waiting before retry...\")\n",
    "            time.sleep(2)  # Wait 2 seconds before retry\n",
    "    \n",
    "    return \"Failed to get response after all retries\"\n",
    "\n",
    "# Test robust calling\n",
    "test_prompt = \"What are the key principles of good software design?\"\n",
    "\n",
    "print(\"üõ°Ô∏è Testing robust LLM calling:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "robust_response = robust_llm_call(test_prompt)\n",
    "print(\"\\nüìã Final Response:\")\n",
    "print(robust_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 7: Summary and Key Takeaways\n",
    "\n",
    "Let's summarize what we've learned and provide key takeaways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö E2E Networks LLM Integration - Summary\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üéØ What we covered:\")\n",
    "print(\"   1. ‚úÖ Custom LangChain LLM wrapper creation\")\n",
    "print(\"   2. ‚úÖ Direct OpenAI-compatible API calls\")\n",
    "print(\"   3. ‚úÖ Performance comparison between methods\")\n",
    "print(\"   4. ‚úÖ Advanced usage examples (code, creative, technical)\")\n",
    "print(\"   5. ‚úÖ Error handling and best practices\")\n",
    "print()\n",
    "print(\"üîë Key Takeaways:\")\n",
    "print(\"   ‚Ä¢ E2E Networks uses OpenAI-compatible API format\")\n",
    "print(\"   ‚Ä¢ LangChain wrapper provides better integration with LangChain ecosystem\")\n",
    "print(\"   ‚Ä¢ Direct API calls offer more control and transparency\")\n",
    "print(\"   ‚Ä¢ Proper error handling is crucial for production use\")\n",
    "print(\"   ‚Ä¢ Temperature and max_tokens significantly affect output\")\n",
    "print()\n",
    "print(\"üöÄ Next Steps:\")\n",
    "print(\"   ‚Ä¢ Integrate with your applications\")\n",
    "print(\"   ‚Ä¢ Experiment with different prompting techniques\")\n",
    "print(\"   ‚Ä¢ Build more sophisticated LLM-powered features\")\n",
    "print(\"   ‚Ä¢ Monitor usage and optimize for your use case\")\n",
    "print()\n",
    "print(\"üéâ You're now ready to use E2E Networks LLM in your projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **E2E Networks Documentation**: Check your E2E Networks dashboard for API documentation\n",
    "- **LangChain Documentation**: https://python.langchain.com/\n",
    "- **OpenAI API Reference**: https://platform.openai.com/docs/api-reference\n",
    "\n",
    "## ü§ù Support\n",
    "\n",
    "If you encounter any issues:\n",
    "1. Check your API credentials and endpoint URL\n",
    "2. Verify network connectivity\n",
    "3. Review error messages carefully\n",
    "4. Contact E2E Networks support for API-related issues\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding with E2E Networks LLM! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}